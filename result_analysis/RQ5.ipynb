{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = \"../output/evaluation/processed\"\n",
    "projects = ['Lang', 'Math', 'Time', 'Closure', 'Cli', 'Compress', 'Codec', 'Collections', 'Csv', \n",
    "    'JacksonCore', 'JacksonXml', 'JxPath', 'Jsoup']\n",
    "featuredir = \"../output/evaluation/features\"\n",
    "seeds = np.arange(0,10,1)\n",
    "decimal_points = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Mutant Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pred(pred_df, decimal_points = 4, with_cnt:bool =False):\n",
    "    from sklearn.metrics import multilabel_confusion_matrix\n",
    "    from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "    out = dict()\n",
    "    # acc\n",
    "    acc = accuracy_score(pred_df.label.values, pred_df.pred_label.values)\n",
    "    acc = np.round(acc, decimals=decimal_points)\n",
    "    line = f\"{acc}\"\n",
    "    # store\n",
    "    out['acc'] = acc\n",
    "\n",
    "    # acc per label \n",
    "    labels = [0,1,2]\n",
    "    cm = multilabel_confusion_matrix(pred_df.label.values, pred_df.pred_label.values, labels = labels)\n",
    "    acc_pclass = []\n",
    "    for i, _ in enumerate(labels):\n",
    "        tn, fp, fn, tp = cm[i].ravel()\n",
    "        acc = (tp + tn)/(tn + fp + fn + tp)\n",
    "        acc = np.round(acc, decimals=decimal_points)\n",
    "        acc_pclass.append(acc)\n",
    "    acc_pclass_l = \"/\".join([str(v) for v in acc_pclass])\n",
    "    line += f\" ({acc_pclass_l})\"\n",
    "    # store\n",
    "    out['acc_pclass'] = acc_pclass\n",
    "\n",
    "    # bal acc\n",
    "    bal_acc = balanced_accuracy_score(pred_df.label.values, pred_df.pred_label.values)\n",
    "    bal_acc = np.round(bal_acc, decimals=decimal_points)\n",
    "    line += f\" & {bal_acc}\"\n",
    "    # store\n",
    "    out['bal_acc'] = bal_acc\n",
    "    \n",
    "    # MAP \n",
    "    aps = []\n",
    "    cnt_none, cnt_all, cnt_studied = 0, 0, 0\n",
    "    for bid, df in pred_df.groupby('bid'):      \n",
    "        bid = int(bid)\n",
    "        predcs = df['prob_0'].values \n",
    "        n_reveal = (df.status == 'reveal').sum()\n",
    "        if n_reveal == 0: cnt_none += 1; continue \n",
    "        elif n_reveal == len(df): cnt_all += 1; continue \n",
    "        cnt_studied += 1\n",
    "        ps, cnt  = [], 0 \n",
    "        status_vs = df.status.values \n",
    "        for i, (status, _) in enumerate(sorted(zip(status_vs, predcs), key = lambda v:v[1], reverse=True)):\n",
    "            if status == 'reveal':\n",
    "                cnt += 1; ps.append(cnt/(i+1))\n",
    "        # compite AP \n",
    "        aps.append(np.mean(ps))\n",
    "    if len(aps) > 0:\n",
    "        map_v= np.round(np.mean(aps), decimals= decimal_points)\n",
    "    else:\n",
    "        map_v = \"-\"\n",
    "    line += f\" & {map_v}\" \n",
    "    # store\n",
    "    out['map'] = map_v\n",
    "\n",
    "    if with_cnt:\n",
    "        line = f\"({cnt_none}/{cnt_all}/{cnt_studied}) & \" + line \n",
    "        # store\n",
    "        out[\"cnt\"] = [cnt_none, cnt_all, cnt_studied]\n",
    "\n",
    "    out['n'] = pred_df[['bid', 'project']].drop_duplicates().shape[0]\n",
    "    return line, out\n",
    "\n",
    "def printout_mdl_perf(outs, decimal_points:int = 3, with_cnt:bool =False):\n",
    "    acc_vs = [out['acc'] for out in outs.values()]\n",
    "    avg_acc = np.round(np.mean(acc_vs), decimals=decimal_points)\n",
    "    #\n",
    "    acc_pclass_vs = np.array([out['acc_pclass'] for out in outs.values()])\n",
    "    avg_acc_pclass = np.round(np.mean(acc_pclass_vs, axis = 0), decimals=2)#decimal_points)\n",
    "    #\n",
    "    bal_acc_vs = [out['bal_acc'] for out in outs.values()]\n",
    "    avg_bal_acc = np.round(np.mean(bal_acc_vs), decimals=decimal_points)\n",
    "    #\n",
    "    map_vs = [out['map'] for out in outs.values()]\n",
    "    if '-' in map_vs:\n",
    "        uniq_map_v = list(set(map_vs))\n",
    "        assert (uniq_map_v[0] == '-') and (len(uniq_map_v) == 1)\n",
    "        avg_map = '-'\n",
    "    else:\n",
    "        avg_map = np.round(np.mean(map_vs), decimals=decimal_points)\n",
    "    line = f\"{avg_acc} ({avg_acc_pclass[0]}/{avg_acc_pclass[1]}/{avg_acc_pclass[2]}) & {avg_bal_acc} & {avg_map}\"\n",
    "    if with_cnt:\n",
    "        cnt_vs = outs[0]['cnt']\n",
    "        cnt_none, cnt_all, cnt_studied = cnt_vs\n",
    "        line = f\"({cnt_none}/{cnt_all}/{cnt_studied}) & \" + line \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_results(pred_dir, seeds, decimal_points, fpath, with_cnt = False):\n",
    "    pred_results = {}\n",
    "    for seed in seeds:\n",
    "        pred_df = pd.read_csv(os.path.join(pred_dir, str(seed), fpath))\n",
    "        pred_results[seed] = pred_df\n",
    "\n",
    "    decimal_points = 3\n",
    "    eval_pred_results = {project:{} for project in projects + ['Total']}\n",
    "    for seed in seeds:\n",
    "        pred_df = pred_results[seed]\n",
    "        pred_df_pproj = pred_df.groupby('project')\n",
    "        for project in projects:\n",
    "            pred_df_proj = pred_df_pproj.get_group(project)\n",
    "            _, rf_output = eval_pred(pred_df_proj, decimal_points = decimal_points, with_cnt=with_cnt)\n",
    "            eval_pred_results[project][seed] = rf_output\n",
    "        _, rf_output = eval_pred(pred_df, decimal_points = decimal_points, with_cnt=with_cnt)\n",
    "        eval_pred_results['Total'][seed] = rf_output\n",
    "    return pred_results, eval_pred_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lang & 27 & 0.89 (0.94/0.91/0.92) & 0.65 & 0.37 & 0.84 (0.94/0.87/0.87) & 0.61 & 0.37 & 0.34 (0.65/0.42/0.6) & 0.33 & 0.33 \\\\\n",
      "Math & 79 & 0.82 (0.9/0.86/0.89) & 0.67 & 0.35 & 0.82 (0.89/0.85/0.89) & 0.63 & 0.31 & 0.34 (0.64/0.48/0.56) & 0.34 & 0.29 \\\\\n",
      "Time & 10 & 0.89 (0.95/0.89/0.94) & 0.38 & 0.92 & 0.89 (0.95/0.89/0.95) & 0.37 & 0.92 & 0.35 (0.68/0.39/0.64) & 0.39 & 0.25 \\\\\n",
      "Closure & 100 & 0.83 (0.99/0.83/0.84) & 0.63 & 0.45 & 0.77 (0.98/0.77/0.79) & 0.52 & 0.31 & 0.34 (0.66/0.42/0.59) & 0.33 & 0.15 \\\\\n",
      "Cli & 22 & 0.86 (0.94/0.87/0.92) & 0.64 & 0.42 & 0.85 (0.94/0.85/0.91) & 0.62 & 0.39 & 0.34 (0.66/0.41/0.61) & 0.35 & 0.45 \\\\\n",
      "Compress & 43 & 0.79 (0.97/0.8/0.8) & 0.59 & 0.3 & 0.78 (0.97/0.79/0.79) & 0.58 & 0.23 & 0.34 (0.66/0.5/0.51) & 0.34 & 0.25 \\\\\n",
      "Codec & 16 & 0.89 (0.97/0.91/0.9) & 0.73 & 0.83 & 0.9 (0.97/0.91/0.91) & 0.76 & 0.81 & 0.33 (0.65/0.48/0.53) & 0.34 & 0.18 \\\\\n",
      "Collections & 2 & 0.98 (1.0/0.98/0.98) & 0.98 & - & 1.0 (1.0/1.0/1.0) & 1.0 & - & 0.38 (0.7/0.38/0.68) & 0.38 & - \\\\\n",
      "Csv & 11 & 0.94 (0.99/0.95/0.94) & 0.6 & - & 0.96 (0.99/0.97/0.96) & 0.62 & - & 0.33 (0.66/0.4/0.61) & 0.29 & - \\\\\n",
      "JacksonCore & 21 & 0.96 (0.98/0.98/0.96) & 0.71 & 0.28 & 0.96 (0.98/0.98/0.97) & 0.73 & 0.33 & 0.34 (0.66/0.63/0.39) & 0.34 & 0.08 \\\\\n",
      "JacksonXml & 3 & 0.66 (1.0/0.66/0.66) & 0.77 & - & 0.76 (1.0/0.76/0.76) & 0.84 & - & 0.36 (0.69/0.58/0.45) & 0.36 & - \\\\\n",
      "JxPath & 9 & 0.85 (1.0/0.86/0.86) & 0.62 & - & 0.85 (1.0/0.85/0.85) & 0.6 & - & 0.34 (0.67/0.37/0.64) & 0.32 & - \\\\\n",
      "Jsoup & 79 & 0.84 (0.95/0.86/0.86) & 0.61 & 0.29 & 0.84 (0.95/0.87/0.87) & 0.6 & 0.26 & 0.34 (0.65/0.56/0.46) & 0.34 & 0.28 \\\\\n",
      "Total & 422 & 0.87 (0.96/0.88/0.89) & 0.67 & 0.34 & 0.85 (0.96/0.87/0.87) & 0.64 & 0.32 & 0.34 (0.66/0.51/0.51) & 0.34 & 0.13 \\\\\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "rf_pred_dir = f\"../output/evaluation/pred/debt_time_thr365/all\"\n",
    "wo_mutop_rf_pred_dir = f\"../output/evaluation/pred/debt_time_thr365/wo_mutop\"\n",
    "rd_pred_dir = f\"../output/evaluation/pred/debt_time_thr365/rd/all\"\n",
    "\n",
    "pred_results, outs_pproj = get_pred_results(rf_pred_dir, seeds, decimal_points, 'rf_pred.csv')\n",
    "_, wo_mut_op_outs_pproj = get_pred_results(wo_mutop_rf_pred_dir, seeds, decimal_points, 'rf_pred.csv')\n",
    "_, rd_outs_pproj = get_pred_results(rd_pred_dir, seeds, decimal_points, 'rd_pred.csv')\n",
    "\n",
    "decimal_points = 2\n",
    "for project in projects + ['Total']:\n",
    "    rf_line = printout_mdl_perf(outs_pproj[project], decimal_points = 2)\n",
    "    wo_mutop_rf_line = printout_mdl_perf(wo_mut_op_outs_pproj[project], decimal_points = 2)\n",
    "    rd_line = printout_mdl_perf(rd_outs_pproj[project], decimal_points = 2)\n",
    "    n = outs_pproj[project][0]['n']\n",
    "    print (project + \" & \" + str(n) + \" & \" + rf_line + \" & \" + wo_mutop_rf_line + \" & \" + rd_line + \" \\\\\\\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
